ğŸ§  DSLR - Logistic Regression Classifier
ğŸ“Œ Project Description

This project implements a generalized logistic regression classifier from scratch in Python to predict a Hogwarts house based on student features.

While logistic regression is typically used for binary classification, we apply a one-vs-all strategy to extend it to a multi-class setting. The implementation is modular and reusable, allowing the classifier to be applied to other datasets beyond this project.

In addition, the project includes a data analysis and visualization section to better understand the dataset and extract meaningful insights before training the model.

ğŸ“Š Methodology
ğŸ” Logistic Regression

Like linear regression, logistic regression uses gradient descent to minimize a loss function. However, it applies a sigmoid activation to map inputs into a probability between 0 and 1, which reflects the likelihood of belonging to a certain class.

ğŸ§® Loss Function - Cross Entropy
J(Î¸)=âˆ’1mâˆ‘i=1m[y(i)logâ¡(hÎ¸(x(i)))+(1âˆ’y(i))logâ¡(1âˆ’hÎ¸(x(i)))]
J(Î¸)=âˆ’
m
1
	â€‹

i=1
âˆ‘
m
	â€‹

[y
(i)
log(h
Î¸
	â€‹

(x
(i)
))+(1âˆ’y
(i)
)log(1âˆ’h
Î¸
	â€‹

(x
(i)
))]

with:

hÎ¸(x)=Ïƒ(Î¸Tx)andÏƒ(z)=11+eâˆ’z
h
Î¸
	â€‹

(x)=Ïƒ(Î¸
T
x)andÏƒ(z)=
1+e
âˆ’z
1
	â€‹

ğŸ“ˆ Gradient Descent

The partial derivative with respect to each parameter 
Î¸j
Î¸
j
	â€‹

 is given by:

âˆ‚Jâˆ‚Î¸j=1mâˆ‘i=1m(hÎ¸(x(i))âˆ’y(i))xj(i)
âˆ‚Î¸
j
	â€‹

âˆ‚J
	â€‹

=
m
1
	â€‹

i=1
âˆ‘
m
	â€‹

(h
Î¸
	â€‹

(x
(i)
)âˆ’y
(i)
)x
j
(i)
	â€‹


This allows for updating the weights during training using gradient descent.

ğŸ—‚ï¸ Program Structure
ğŸ“„ describe.py

Usage:
